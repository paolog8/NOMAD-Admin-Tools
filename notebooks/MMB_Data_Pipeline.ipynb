{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fa257cfe",
   "metadata": {},
   "source": [
    "# MMB Data Pipeline\n",
    "\n",
    "This notebook implements a structured data pipeline for Mini Module Baseline (MMB) data from NOMAD. It follows a logical workflow:\n",
    "\n",
    "1. **Setup**: Import libraries and configure environment\n",
    "2. **Authentication**: Connect to NOMAD API with proper credentials\n",
    "3. **Data Retrieval**: Fetch MMB-related samples and archive data\n",
    "4. **Relationship Analysis**: Identify related entries and references\n",
    "5. **Data Processing**: Transform raw data into structured DataFrames\n",
    "6. **Analysis & Visualization**: Prepare data for modeling and analysis\n",
    "\n",
    "Date: June 27, 2025"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "093f5495",
   "metadata": {},
   "source": [
    "## 1. Setup and Environment Configuration\n",
    "\n",
    "First, we'll import all necessary libraries and set up the environment for working with NOMAD API and data processing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d0f2e8c3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded environment from: /home/qkg/Documents/1_PROJECTS/NOMAD-Tools/NOMAD-Admin-Tools/.env\n"
     ]
    }
   ],
   "source": [
    "# Ensure we can load the .env file\n",
    "from pathlib import Path\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "# Find the .env file in the project root (two levels up from this notebook)\n",
    "env_path = Path().absolute().parent / '.env'\n",
    "if env_path.exists():\n",
    "    load_dotenv(dotenv_path=env_path)\n",
    "    print(f\"Loaded environment from: {env_path}\")\n",
    "else:\n",
    "    print(f\"Warning: No .env file found at {env_path}\")\n",
    "\n",
    "# Now we can import the rest of our dependencies\n",
    "import os\n",
    "import sys\n",
    "import json\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from datetime import datetime\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "# Import NOMAD API modules\n",
    "sys.path.append('../')\n",
    "from nomad_api.auth import authenticate, OASIS_OPTIONS\n",
    "from nomad_api.client import NomadClient\n",
    "from nomad_api.data import query_sample_entries,get_all_samples_with_authors\n",
    "\n",
    "# Set plotting style\n",
    "plt.style.use('seaborn-v0_8-whitegrid')\n",
    "sns.set_context(\"notebook\", font_scale=1.2)\n",
    "\n",
    "# Display settings for pandas\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.max_rows', 50)\n",
    "pd.set_option('display.width', 1000)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f18309d6",
   "metadata": {},
   "source": [
    "## 2. Authentication with NOMAD API\n",
    "\n",
    "To access NOMAD data, we need to authenticate with the API. The authentication will use credentials from the environment variables or prompt for them if not available."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b7793a82",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully authenticated as: Paolo Graniero\n"
     ]
    }
   ],
   "source": [
    "# Use the SE Oasis URL - this can be changed to other OASIS options if needed\n",
    "OASIS_URL = OASIS_OPTIONS['SE Oasis']\n",
    "\n",
    "# The authenticate function will automatically try to:\n",
    "# 1. Use NOMAD_CLIENT_ACCESS_TOKEN if available\n",
    "# 2. Fall back to NOMAD_USERNAME and NOMAD_PASSWORD from .env file\n",
    "# 3. Prompt for credentials if neither are available\n",
    "token, user_info = authenticate(base_url=OASIS_URL)\n",
    "\n",
    "print(f\"Successfully authenticated as: {user_info.get('name', user_info.get('username'))}\")\n",
    "\n",
    "# Create the client with the obtained token\n",
    "client = NomadClient(base_url=OASIS_URL, token=token)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b8d0b43",
   "metadata": {},
   "source": [
    "## 3. Data Retrieval\n",
    "\n",
    "In this section, we'll retrieve all relevant MMB data from NOMAD:\n",
    "1. Fetch all HySprint samples\n",
    "2. Filter for MMB-related samples\n",
    "3. Extract metadata about the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "223d6390",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Attempting to retrieve samples with admin access...\n",
      "Admin access failed, falling back to visible access...\n",
      "Attempting to retrieve samples with visible access...\n",
      "Found 1513 samples (approximately 2 pages)\n",
      "Processed page 1/2\n",
      "Processed page 2/2\n",
      "Total HySprint_Sample entries found: 1513\n",
      "Unique upload names found: 82\n",
      "MMB uploads found: 23\n",
      "MMB upload names:\n",
      "- HZB_MMB_8_2001\n",
      "- MMB Batch 12.0\n",
      "- MMB Batch 12.1\n",
      "- MMB Batch 12.10\n",
      "- MMB Batch 12.11\n",
      "- MMB Batch 12.12\n",
      "- MMB Batch 12.5\n",
      "- MMB Batch 12.7\n",
      "- MMB Batch 12.8\n",
      "- MMB Batch 12.9\n",
      "- MMB Batch 13.0\n",
      "- MMB Batch 14.0\n",
      "- MMB Batch 15.0\n",
      "- MMB Batch 16.0\n",
      "- MMB Batch 17.0\n",
      "- MMB Batch 19.0\n",
      "- MMB Batch 20.0\n",
      "- MMB Batch 2000\n",
      "- MMB Batch 2002\n",
      "- MMB Batch 21.0\n",
      "- MMB Batch 23.0\n",
      "- MMB Batch 24.0\n",
      "- MMB Batch 25.0\n",
      "Total MMB samples found: 483\n",
      "\n",
      "Sample entry summary:\n",
      "- entry_id: -635Z62MBAoFkXDxqYfamU3qcgCX\n",
      "- upload_id: Uq7aoxCCRKe0g2sprkDbMg\n",
      "- upload_name: MMB Batch 2002\n",
      "- entry_name: None\n"
     ]
    }
   ],
   "source": [
    "# Use query_sample_entries to fetch all samples\n",
    "# This function handles pagination and admin/visible access automatically\n",
    "\n",
    "# Step 1: Fetch all HySprint samples\n",
    "all_samples = get_all_samples_with_authors(\n",
    "    client=client,\n",
    "    section_type=\"HySprint_Sample\",\n",
    "    page_size=1000  # Adjust based on your needs\n",
    ")\n",
    "print(f\"Total HySprint_Sample entries found: {len(all_samples)}\")\n",
    "\n",
    "# Step 2: Extract and filter upload names\n",
    "unique_upload_names = list(set(sample['upload_name'] for sample in all_samples if 'upload_name' in sample))\n",
    "unique_upload_names = [name for name in unique_upload_names if name]  # Filter out empty names\n",
    "print(f\"Unique upload names found: {len(unique_upload_names)}\")\n",
    "\n",
    "# Step 3: Filter for MMB-related uploads\n",
    "mmb_uploads_names = [name for name in unique_upload_names if \"MMB\" in name]\n",
    "print(f'MMB uploads found: {len(mmb_uploads_names)}')\n",
    "print(f'MMB upload names:')\n",
    "for name in sorted(mmb_uploads_names):\n",
    "    print(f\"- {name}\")\n",
    "\n",
    "# Step 4: Get all MMB samples\n",
    "mmb_samples = [sample for sample in all_samples if sample.get('upload_name') in mmb_uploads_names]\n",
    "print(f\"Total MMB samples found: {len(mmb_samples)}\")\n",
    "\n",
    "# Display a brief summary of the first sample (if available)\n",
    "if mmb_samples:\n",
    "    sample_summary = {\n",
    "        \"entry_id\": mmb_samples[0].get('entry_id'),\n",
    "        \"upload_id\": mmb_samples[0].get('upload_id'),\n",
    "        \"upload_name\": mmb_samples[0].get('upload_name'),\n",
    "        \"entry_name\": mmb_samples[0].get('entry_name'),\n",
    "    }\n",
    "    print(\"\\nSample entry summary:\")\n",
    "    for key, value in sample_summary.items():\n",
    "        print(f\"- {key}: {value}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9384db0f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unique upload names found: 82\n",
      "Upload names:\n",
      "- 1st_Batch_HySPRINT_Yuxin\n",
      "- 1st_batch_IRIS_Yuxin\n",
      "- 2nd_Batch_IRIS_Yuxin\n",
      "- AF_SDC_MAPI_ink_B4\n",
      "- AF_SDC_MAPIink_Batch7\n",
      "- Batch VII - SAM Wettability\n",
      "- Batch_5_CSMB_Yuxin\n",
      "- Calender Week 15 2024 Module Baseline\n",
      "- HZB_MMB_8_2001\n",
      "- IJP-BL_01\n",
      "- Introduction to Nomad - Workshop Material KJ\n",
      "- KW10 Module Baseline\n",
      "- KW15 FACs and PEtOx60\n",
      "- KW16 FACs and PEtOx\n",
      "- KW22 PEtOx60 in FACs with MACl\n",
      "- KW25 POx in FACs - Polymer Variation I\n",
      "- MAFA Batch 5\n",
      "- MAFA Batch 8\n",
      "- MAFA Batch 9\n",
      "- MAFA Batch2\n",
      "- MAPI ink_ref_spin_coated\n",
      "- MMB Batch 12.0\n",
      "- MMB Batch 12.1\n",
      "- MMB Batch 12.10\n",
      "- MMB Batch 12.11\n",
      "- MMB Batch 12.12\n",
      "- MMB Batch 12.5\n",
      "- MMB Batch 12.7\n",
      "- MMB Batch 12.8\n",
      "- MMB Batch 12.9\n",
      "- MMB Batch 13.0\n",
      "- MMB Batch 14.0\n",
      "- MMB Batch 15.0\n",
      "- MMB Batch 16.0\n",
      "- MMB Batch 17.0\n",
      "- MMB Batch 19.0\n",
      "- MMB Batch 20.0\n",
      "- MMB Batch 2000\n",
      "- MMB Batch 2002\n",
      "- MMB Batch 21.0\n",
      "- MMB Batch 23.0\n",
      "- MMB Batch 24.0\n",
      "- MMB Batch 25.0\n",
      "- MMX B7\n",
      "- SDC-PSC-7_8_Dec2023_b2\n",
      "- SOP-02_20241010_TM\n",
      "- SOP_CSMB_TM_09052025\n",
      "- Semi-transparent_Batch_1\n",
      "- Slot-die coated solar cells\n",
      "- Slot-die coated solar cells_07Dec2023\n",
      "- Slot-die coated solar cells_31_10_2023\n",
      "- SolarCellsBaseline-Batch_12\n",
      "- SolarCellsBaseline-Batch_12-3\n",
      "- Sun-PLI Test\n",
      "- Test\n",
      "- Test for Alexander\n",
      "- Tianmiao-Baseline precursor reference\n",
      "- Upcycling_PbI2_initial_test_2024\n",
      "- Week July 23\n",
      "- Yanyan 1.53 ev-batch 80\n",
      "- Yanyan Batch 81-normal recipe\n",
      "- Yanyan Batch 96\n",
      "- Yanyan Batch86\n",
      "- Yanyan Batch88\n",
      "- Yanyan Batch94 1.54 for outdoor\n",
      "- Yanyan Batch95 1.54 eV\n",
      "- Yanyan Batch97\n",
      "- Yanyan SOLARTAP sop batch 79\n",
      "- Yanyan Solartap recipe-batch 42 practice with HZB chemicals\n",
      "- Yanyan Solartap recipe-batch 46 First batch for training Yuxin 20250912\n",
      "- Yanyan batch 83 large area\n",
      "- Yanyan batch 84\n",
      "- Yanyan batch 89\n",
      "- YiSh_1_excess macl_niox_ph-4pacz\n",
      "- Zihan_batch1\n",
      "- Zihan_batch2\n",
      "- Zihan_batch3\n",
      "- Zihan_batch4\n",
      "- Zihan_batch5\n",
      "- Zihan_batch6\n",
      "- excess macl, different annealing temperature\n",
      "- yanyan-solarTAP batch 82\n"
     ]
    }
   ],
   "source": [
    "unique_upload_names = list(set(sample['upload_name'] for sample in all_samples if 'upload_name' in sample))\n",
    "unique_upload_names = [name for name in unique_upload_names if name]  # Filter out empty names\n",
    "\n",
    "print(f\"Unique upload names found: {len(unique_upload_names)}\")\n",
    "print('Upload names:')\n",
    "for name in sorted(list(unique_upload_names)):\n",
    "    print(f\"- {name}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9e787b51",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total MMB samples found: 483\n",
      "Example MMB sample: {\n",
      "  \"entry_id\": \"-635Z62MBAoFkXDxqYfamU3qcgCX\",\n",
      "  \"upload_id\": \"Uq7aoxCCRKe0g2sprkDbMg\",\n",
      "  \"lab_id\": \"HZB_MMB_8-2002-3-0\",\n",
      "  \"main_author\": \"df8bc696-58aa-4571-95fb-d71a800e1c07\",\n",
      "  \"coauthors\": [],\n",
      "  \"coauthor_groups\": [\n",
      "    \"MjM4ze-URpu0NrHBulkRYg\"\n",
      "  ],\n",
      "  \"upload_create_time\": \"2025-03-19T11:06:57.233000\",\n",
      "  \"published\": false,\n",
      "  \"license\": \"CC BY 4.0\",\n",
      "  \"upload_name\": \"MMB Batch 2002\"\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "mmb_samples = [sample for sample in all_samples if sample.get('upload_name') in mmb_uploads_names]\n",
    "print(f\"Total MMB samples found: {len(mmb_samples)}\")\n",
    "print(f'Example MMB sample: {json.dumps(mmb_samples[0], indent=2)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d52882fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to get archive data for a specific entry\n",
    "def get_sample_archive(client, entry_id):\n",
    "    \"\"\"\n",
    "    Retrieve the complete archive data for a specific entry using the NOMAD API.\n",
    "    \n",
    "    Args:\n",
    "        client (NomadClient): Authenticated NOMAD client\n",
    "        entry_id (str): The entry ID of the sample\n",
    "        \n",
    "    Returns:\n",
    "        dict: The complete archive data for the entry\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Prepare the request body\n",
    "        request_body = {\n",
    "            \"required\": \"*\"\n",
    "        }\n",
    "        \n",
    "        # Use the make_request method with the correct endpoint pattern and request body\n",
    "        response = client.make_request(\n",
    "            'post',\n",
    "            f'entries/{entry_id}/archive/query',\n",
    "            json_data=request_body\n",
    "        )\n",
    "        return response\n",
    "    except Exception as e:\n",
    "        print(f\"Error retrieving archive data: {e}\")\n",
    "        return None\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ad5907b",
   "metadata": {},
   "outputs": [],
   "source": [
    "archive_data['data']['archive']['m_ref_archives']xx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "129b9585",
   "metadata": {},
   "outputs": [],
   "source": [
    "archive_data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "314be5c4",
   "metadata": {},
   "source": [
    "## 5. Relationship Analysis\n",
    "\n",
    "MMB data in NOMAD has complex relationships between entries. This section provides functions to trace relationships between entries and understand the data provenance.\n",
    "\n",
    "Let's create a function to find all entries that reference a specific target entry ID. This will help us track the relationships between different entries in the database."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "bf44ce54",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to get entries that reference a specific target entry\n",
    "def get_referencing_entries(client, target_entry_id):\n",
    "    \"\"\"\n",
    "    Find all entries that reference a specific target entry.\n",
    "    \n",
    "    Args:\n",
    "        client (NomadClient): Authenticated NOMAD client\n",
    "        target_entry_id (str): The entry ID to search for in references\n",
    "        \n",
    "    Returns:\n",
    "        list: List of entries that reference the target entry\n",
    "    \"\"\"\n",
    "    # Construct the query to search for entries with matching target_entry_id in references\n",
    "    query = {\n",
    "        \"owner\": \"visible\",\n",
    "        \"query\": {\n",
    "            \"entry_references.target_entry_id\": target_entry_id\n",
    "        }\n",
    "    }\n",
    "\n",
    "    try:\n",
    "        # Use the make_request method to query the entries\n",
    "        response = client.make_request('post', 'entries/query', json_data=query)\n",
    "        if response and 'data' in response:\n",
    "            return response['data']\n",
    "        return []\n",
    "    except Exception as e:\n",
    "        print(f\"Error searching for referencing entries: {e}\")\n",
    "        return []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualizing the processed data\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Function to create visualizations based on the available data\n",
    "def visualize_mmb_data(wide_df=None, narrow_df=None):\n",
    "    \"\"\"\n",
    "    Create visualizations for MMB data in either wide or narrow format.\n",
    "    \n",
    "    Args:\n",
    "        wide_df (DataFrame): Wide format DataFrame (optional)\n",
    "        narrow_df (DataFrame): Narrow format DataFrame (optional)\n",
    "    \"\"\"\n",
    "    plt.figure(figsize=(12, 8))\n",
    "    \n",
    "    # Check if we have wide format data\n",
    "    if wide_df is not None and not wide_df.empty:\n",
    "        # Example 1: Distribution of layer types\n",
    "        if 'layer_type' in wide_df.columns:\n",
    "            plt.subplot(2, 2, 1)\n",
    "            layer_counts = wide_df['layer_type'].value_counts()\n",
    "            sns.barplot(x=layer_counts.index, y=layer_counts.values)\n",
    "            plt.title('Distribution of Layer Types')\n",
    "            plt.xlabel('Layer Type')\n",
    "            plt.ylabel('Count')\n",
    "            plt.xticks(rotation=45)\n",
    "        \n",
    "        # Example 2: Annealing temperature by process name\n",
    "        if 'annealing_temperature' in wide_df.columns and 'process_name' in wide_df.columns:\n",
    "            plt.subplot(2, 2, 2)\n",
    "            sns.boxplot(x='process_name', y='annealing_temperature', data=wide_df)\n",
    "            plt.title('Annealing Temperature by Process')\n",
    "            plt.xlabel('Process Name')\n",
    "            plt.ylabel('Temperature (°C)')\n",
    "            plt.xticks(rotation=45)\n",
    "    \n",
    "    # Check if we have narrow format data\n",
    "    if narrow_df is not None and not narrow_df.empty:\n",
    "        # Example 3: Parameter value distribution\n",
    "        if 'parameter_name' in narrow_df.columns and 'parameter_value' in narrow_df.columns:\n",
    "            # Filter for numeric parameters only\n",
    "            numeric_params = narrow_df[pd.to_numeric(narrow_df['parameter_value'], errors='coerce').notna()]\n",
    "            \n",
    "            if not numeric_params.empty:\n",
    "                # Get top 5 most frequent parameters\n",
    "                top_params = numeric_params['parameter_name'].value_counts().nlargest(5).index.tolist()\n",
    "                \n",
    "                # Filter for top parameters\n",
    "                top_param_data = numeric_params[numeric_params['parameter_name'].isin(top_params)]\n",
    "                \n",
    "                if not top_param_data.empty:\n",
    "                    plt.subplot(2, 2, 3)\n",
    "                    sns.boxplot(x='parameter_name', y='parameter_value', data=top_param_data)\n",
    "                    plt.title('Distribution of Top Parameter Values')\n",
    "                    plt.xlabel('Parameter')\n",
    "                    plt.ylabel('Value')\n",
    "                    plt.xticks(rotation=45)\n",
    "                    \n",
    "                    # Example 4: Parameter values by sample\n",
    "                    plt.subplot(2, 2, 4)\n",
    "                    pivot_data = top_param_data.pivot_table(\n",
    "                        index='sample_lab_id', \n",
    "                        columns='parameter_name', \n",
    "                        values='parameter_value',\n",
    "                        aggfunc='mean'\n",
    "                    )\n",
    "                    sns.heatmap(pivot_data, annot=True, cmap='viridis', fmt='.2f')\n",
    "                    plt.title('Parameter Values by Sample')\n",
    "                    plt.ylabel('Sample ID')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# Try to visualize the data if available\n",
    "try:\n",
    "    # Get wide format DataFrame if available\n",
    "    wide_format_df = None\n",
    "    if 'wide_df' in globals() and not (isinstance(wide_df, pd.DataFrame) and wide_df.empty):\n",
    "        wide_format_df = wide_df\n",
    "    \n",
    "    # Get narrow format DataFrame if available\n",
    "    narrow_format_df = None\n",
    "    if 'narrow_df' in globals() and not (isinstance(narrow_df, pd.DataFrame) and narrow_df.empty):\n",
    "        narrow_format_df = narrow_df\n",
    "    \n",
    "    # Create visualizations\n",
    "    if wide_format_df is not None or narrow_format_df is not None:\n",
    "        visualize_mmb_data(wide_format_df, narrow_format_df)\n",
    "    else:\n",
    "        print(\"No processed DataFrames available for visualization.\")\n",
    "        print(\"Run the data processing cells first to generate 'wide_df' and/or 'narrow_df'.\")\n",
    "except NameError as e:\n",
    "    print(f\"Missing variable: {e}\")\n",
    "    print(\"Run the data processing cells first to generate the necessary DataFrames.\")\n",
    "except Exception as e:\n",
    "    print(f\"Error during visualization: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Summary and Next Steps\n",
    "\n",
    "This notebook has demonstrated a complete data pipeline for Mini Module Baseline (MMB) data from NOMAD:\n",
    "\n",
    "1. **Authentication** with NOMAD API using the proper credentials\n",
    "2. **Data Retrieval** to fetch all MMB-related samples and their metadata\n",
    "3. **Archive Data Access** to get detailed information for specific samples\n",
    "4. **Relationship Analysis** to understand the connections between entries\n",
    "5. **Data Processing** to transform raw data into structured formats for analysis\n",
    "6. **Visualization** to gain insights from the processed data\n",
    "\n",
    "### Next Steps\n",
    "\n",
    "To extend this pipeline, you could:\n",
    "\n",
    "1. **Implement batch processing** to handle all MMB samples at once\n",
    "2. **Add more advanced visualizations** specific to your research questions\n",
    "3. **Develop machine learning models** to predict properties or optimize processes\n",
    "4. **Create interactive dashboards** for sharing insights with collaborators\n",
    "5. **Set up automated data update pipelines** for continuous monitoring\n",
    "\n",
    "### Usage Guide\n",
    "\n",
    "1. Ensure you have proper authentication credentials in your `.env` file\n",
    "2. Run all cells in sequence to set up the complete pipeline\n",
    "3. Use the `get_all_samples_with_authors()` function to fetch relevant samples\n",
    "4. Process the data using the provided transformation functions\n",
    "5. Visualize results using the visualization tools\n",
    "\n",
    "For any issues or questions, refer to the NOMAD API documentation or contact the NOMAD team."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
